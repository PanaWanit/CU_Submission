{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1049b6-5a52-49f9-b259-9ff1796fd026",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLE and Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d50be-3950-47be-a9cb-721d6b1b3e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## T1 and OT1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e4ae4-2d3e-425e-801d-09e60f308ff3",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\arg \\max_\\alpha&\\ Pr(y_N, y_{N-1}, \\ldots, y_1, y_0 ; \\alpha) \\\\\n",
    "= \\arg \\max_\\alpha&\\ Pr(\\bigcap_{i=0}^{N}y_i; \\alpha) \\\\\n",
    "= \\arg \\max_\\alpha&\\ Pr(y_N | \\bigcap_{i=0}^{N-1}y_i; a)Pr(\\bigcap_{i=0}^{N-1}y_i; a)\\\\\n",
    "= \\arg \\max_\\alpha&\\ Pr(y_0; \\alpha) \\prod_{i=1}^N Pr(y_i|\\bigcap_{j=0}^{i-1}y_j ; \\alpha) & \\quad \\left( \\text{Apply probabilistic Chain rule} \\right) \\\\\n",
    "= \\arg \\max_\\alpha&\\ Pr(y_0; \\alpha)\\prod_{i=1}^NPr(y_i|y_{i-1}; \\alpha) & \\quad \\left( \\text{$y_i$ is Markov Process} \\right) \\\\\n",
    "= \\arg \\max_\\alpha&\\ Pr(y_0)\\prod_{i=1}^NPr(y_i|y_{i-1}; \\alpha) & \\quad \\left(y_0 \\text{ is independent to } \\alpha\\right)\\\\\n",
    "= \\arg \\max_\\alpha&\\ \\mathcal{N}\\left(y_0; 0, \\lambda \\right)\\prod_{i=1}^N\\mathcal{N}\\left(y_i; \\alpha y_{i-1}, \\sigma^2\\right) \\\\\n",
    "= \\arg \\max_\\alpha&\\  \\log\\mathcal{N}\\left(y_0; 0, \\lambda \\right) + \\sum_{i=1}^N \\log \\mathcal{N}\\left(y_i; \\alpha y_{i-1}, \\sigma^2\\right) & \\quad \\left(\\text{Take log on likelihood} \\right) \\\\\n",
    "= \\arg \\max_\\alpha&\\ \\log\\frac{1}{\\sqrt{2\\pi\\lambda}} + N\\log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}+ \\frac{-y_0^2}{2\\lambda} + \\sum_{i=1}^N\\frac{-(y_i-\\alpha y_{i-1})^2}{2\\sigma^2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "$\\text{Take Derivative to calculate argument max}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\alpha} \\left( \\log\\frac{1}{\\sqrt{2\\pi\\lambda}} + N\\log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}+ \\frac{-y_0^2}{2\\lambda} + \\sum_{i=1}^N\\frac{-(y_i-\\alpha y_{i-1})^2}{2\\sigma^2} \\right) &= 0 \\\\\n",
    "\\sum_{i=1}^N\\frac{-2(y_i-\\alpha y_{i-1})}{2\\sigma^2}\\frac{\\partial}{\\partial \\alpha}(y_i-\\alpha y_{i-1})  &= 0 \\\\\n",
    "\\sum_{i=1}^N\\frac{-2(y_i-\\alpha y_{i-1})}{2\\sigma^2}(-y_{i-1})  &= 0 \\\\\n",
    "\\sum_{i=1}^N(y_i-\\alpha y_{i-1})y_{i-1} &= 0 \\\\\n",
    "\\sum_{i=1}^Ny_i \\cdot y_{i-1} - \\alpha \\sum_{i=1}^Ny_{i-1}^2 &= 0 \\\\\n",
    "\\therefore \\alpha = \\frac{\\sum_{i=1}^Ny_i \\cdot y_{i-1}}{\\sum_{i=0}^{N-1}y_{i}^2}\n",
    "\\end{align*}\n",
    "\n",
    "$\\text{For } \\textbf{T1} \\text{ the answer is}$\n",
    "$$\\alpha = \\frac{y_2y_1 + y_1y_0}{y_1^2 + y_0^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa171fb-81c7-4c10-8e61-eea12653130b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## T2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c89f7-eb37-44c7-b4fa-3028909a26b7",
   "metadata": {},
   "source": [
    " \n",
    "\\begin{align*}\n",
    "P(w_1 | x) &= P(w_2 | x) \\\\\n",
    "P(x | w_1)P(w_1) &= P(x | w_2)P(w_2) \\\\\n",
    "P(x | w_1) &= P(x | w_2) & \\because P(w_1)=P(w_2) \\\\\n",
    "\\mathcal{N}(x; 4, 2) &= \\mathcal{N}(x; 0, 2) \\\\\n",
    "\\frac{1}{\\sqrt{2\\pi\\cdot 2}}\\exp\\left( \\frac{-(x-4)^2}{2 \\cdot 2} \\right) &= \\frac{1}{\\sqrt{2\\pi\\cdot 2}}\\exp\\left( \\frac{-(x-0)^2}{2 \\cdot 2} \\right) \\\\\n",
    "(x-4)^2 &= x^2 \\\\\n",
    "(2x-4)(-4) &= 0 \\\\\n",
    "x &= 2\n",
    "\\end{align*}\n",
    "\n",
    "The decision boundary is $x=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edebc5a-ebd4-4dfd-bec3-cf1afe715b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40019b-cdcf-4795-b752-a0e7568d3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9e29f-1e06-482e-a91d-c6f1358eff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4.0, 8.0, 200)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.xlim((-4, 8))\n",
    "# w1 : N(4, 2)\n",
    "plt.plot(x, norm.pdf(x, 4, np.sqrt(2)), label = 'w1')\n",
    "plt.fill_between(x, norm.pdf(x, 4, np.sqrt(2)), where = (x<=2), color='steelblue')\n",
    "# w2: N(0, 2)\n",
    "plt.plot(x, norm.pdf(x, 0, np.sqrt(2)), label = 'w2')\n",
    "plt.fill_between(x, norm.pdf(x, 0, np.sqrt(2)), where = (x>=2), color='orange')\n",
    "\n",
    "\n",
    "#  decision boundary x = 2\n",
    "plt.axvline(x=2, ymax= norm.pdf(2, 0, np.sqrt(2)) / norm.pdf(0, 0, np.sqrt(2)) , color='r', linestyle='--', label='decision boundary')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25c972-3074-4352-a476-9b658dfec832",
   "metadata": {
    "tags": []
   },
   "source": [
    "## T3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4c999-692a-451f-8374-18eae071bcf8",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "P(w_1 | x) &= P(w_2 | x) \\\\\n",
    "P(x | w_1)P(w_1) &= P(x | w_2)P(w_2) \\\\\n",
    "P(x | w_1)(0.75) &= P(x | w_2)(0.25) \\\\\n",
    "3 &= \\frac{\\mathcal{N}(x; 0, 2)}{\\mathcal{N}(x; 4, 2)} \\\\\n",
    "3 &= \\frac{\\exp(-x^2/4)}{ \\exp(-(x-4)^2 / 4)} \\\\\n",
    "4\\ln 3 &= (x-4)^2-x^2 \\\\\n",
    "4\\ln 3 &= (2x-4)(-4) \\\\\n",
    "x &= \\frac{4-\\ln 3}{2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "$\\therefore$ The decision boundary is $x = \\frac{4-\\ln 3}{2} \\approx 1.45069$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca0428-3974-45a8-9dab-a4e80143c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4.0, 8.0, 200)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.xlim((-4, 8))\n",
    "\n",
    "boundary = (4 - np.log(3)) / 2\n",
    "\n",
    "# w1 : N(4, 2)\n",
    "plt.plot(x, 0.75 * norm.pdf(x, 4, np.sqrt(2)), label = 'w1')\n",
    "plt.fill_between(x, 0.75 * norm.pdf(x, 4, np.sqrt(2)), where = (x<=boundary), color='steelblue')\n",
    "# w2: N(0, 2)\n",
    "plt.plot(x, 0.25 * norm.pdf(x, 0, np.sqrt(2)), label = 'w2')\n",
    "plt.fill_between(x, 0.25 * norm.pdf(x, 0, np.sqrt(2)), where = (x>=boundary), color='orange')\n",
    "#  decision boundary x = 2\n",
    "\n",
    "plt.axvline(x=boundary, ymax=  norm.pdf(boundary, 4, np.sqrt(2))  / norm.pdf(4, 4, np.sqrt(2)) + 0.025, color='g', linestyle='--', label='new decision boundary')\n",
    "plt.axvline(x=2, ymax=norm.pdf(2, 0, np.sqrt(2)) / norm.pdf(4, 4, np.sqrt(2)) , color='r', linestyle='--', label='old decision boundary')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb47cc-5e57-42f7-b9d7-854f31262b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OT2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676aeab-e264-4b9e-9b46-6f8a7ec8809a",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathcal{N}(x; \\mu_1, \\sigma^2) &= \\mathcal{N}(x; \\mu_2, \\sigma^2) \\\\\n",
    "\\exp\\left(-\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right) &= \\exp\\left(-\\frac{(x-\\mu_2)^2}{2\\sigma^2}\\right) & \\because \\sigma \\text{ is equal}\\\\\n",
    "(x-\\mu_1)^2 &= (x-\\mu_2)^2 \\\\\n",
    "(x-\\mu_1)^2 - (x-\\mu_2)^2 &= 0 \\\\\n",
    "(2x-\\mu_1-\\mu_2)(\\mu_2 - \\mu_1) &= 0 \\\\\n",
    "x &= \\frac{\\mu_1 + \\mu_2}{2} & \\text{where } \\mu_1 \\ne \\mu_2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b9015-f25e-4bf3-b7cf-2577b99a5710",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OT3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d53b8-2f13-4f48-81fa-0bdab1535ab2",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathcal{N}(x;4, 2) &= \\mathcal{N}(x;0, 4) \\\\\n",
    "\\frac{1}{\\sqrt{2\\pi\\cdot 2}}\\exp\\left( \\frac{-(x-4)^2}{2 \\cdot 2} \\right) &= \\frac{1}{\\sqrt{2\\pi\\cdot 4}}\\exp\\left( \\frac{-(x-0)^2}{2 \\cdot 4} \\right) \\\\\n",
    "\\sqrt{2} &= \\exp\\left(-\\frac{x^2}{8} + \\frac{(x-4)^2}{4}\\right)\\\\\n",
    "\\frac{1}{2}\\ln2 &= -\\frac{x^2}{8} + \\frac{(x-4)^2}{4} \\\\\n",
    "4\\ln2 &= 2(x-4)^2 - x^2 \\\\\n",
    "x^2 - 16x + (32 - 4\\ln2) &= 0 \\\\\n",
    "x &\\approx 2.10317, 13.8968 & \\because x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}\n",
    "\\end{align*}\n",
    "\n",
    "$\\text{Choose } x = 2.10317 \\text{ because } 0 \\le 2.10317 \\le 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cff2c-28aa-4e61-96a0-6828734e3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4.0, 8.0, 200)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.xlim((-4, 8))\n",
    "\n",
    "boundary = 2.10317\n",
    "# w1 : N(4, 2)\n",
    "plt.plot(x, stats.norm.pdf(x, 4, np.sqrt(2)), label = 'w1')\n",
    "plt.fill_between(x, norm.pdf(x, 4, np.sqrt(2)), where = (x<=boundary), color='steelblue')\n",
    "# w2: N(0, 4)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, np.sqrt(4)), label = 'w2')\n",
    "plt.fill_between(x, norm.pdf(x, 0, np.sqrt(4)), where = (x>=boundary), color='orange')\n",
    "#  decision boundary x = 2\n",
    "\n",
    "plt.axvline(x=boundary, ymax=norm.pdf(boundary, 4, np.sqrt(2)) / norm.pdf(4, 4, np.sqrt(2)) , color='r', linestyle='--', label='decision boundary')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f06ebf-25d9-45fb-bec8-17201c51a00b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Employee Attrition Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac3a32-7c52-4921-814a-9b39767bf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hr-employee-attrition-with-null.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70b94f-1f74-4c9e-8f6d-e39f53e57740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Attrition\"] == \"no\", \"Attrition\"] = 0.0\n",
    "df.loc[df[\"Attrition\"] == \"yes\", \"Attrition\"] = 1.0\n",
    "\n",
    "cat_cols = ['Department', 'Attrition', 'BusinessTravel', 'EducationField', 'Gender', 'JobRole',\n",
    "                              'MaritalStatus', 'OverTime']\n",
    "for col in cat_cols:\n",
    "    df[col] = pd.Categorical(df[col]).codes\n",
    "    df.loc[df[col] == -1, col] = np.nan\n",
    "    \n",
    "df = df.loc[:, ~df.columns.isin(['EmployeeNumber', 'Unnamed: 0', 'EmployeeCount', 'StandardHours', 'Over18'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad551fe-f270-496c-9379-6238561ed9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ac37d-2930-453e-b0a9-d2f6e455f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.1, random_state = 42, stratify = df['Attrition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f164524-bd18-48a2-8017-cd3259c8d96f",
   "metadata": {},
   "source": [
    "## T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864de203-818b-4f8b-acef-7133712f3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Age', 'MonthlyIncome', 'DistanceFromHome']\n",
    "fig, ax = plt.subplots(3, figsize=(10, 15))\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    train_col_no_nan = df_train[~df_train[col].isna()][col]\n",
    "    hist, bin_edge = np.histogram(train_col_no_nan, 40)\n",
    "    \n",
    "    print(f'{col} column has {(hist == 0).sum()} bins zero counts ')\n",
    "    \n",
    "    ax[i].set_title(col) \n",
    "    ax[i].fill_between(bin_edge.repeat(2)[1:-1], hist.repeat(2), facecolor='steelblue')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc5035-dedf-4601-9ee9-6b4ed64f312c",
   "metadata": {},
   "source": [
    "**Age** and **MonthlyIncome** have a good discretization because there is no sparse in data, <br>\n",
    "On the other hand, **DistanceFromHome** has total 11 empty bins the sparse of data in this feature. <br>\n",
    "The test data have a chance to appear in the empty bins so that will make the probability value zero therefore the DistanceFromHome feature is bad discretization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4e3b1-29a4-4a20-b35d-6d82e808863a",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5f1d2-af33-4469-bb6d-89878b92edec",
   "metadata": {},
   "source": [
    "We can you Gaussian estimate for an Age feature because the histogram looks like it is a Gaussian distribution.<br>\n",
    "The Monthly Income and DistanceFromHome have the right skewness so it is not good to eliminate with the Gaussian.<br>\n",
    "\n",
    "\n",
    "the **Gaussian Mixture Model (GMM)** can estimate all feature include  **Monthly Income** and **DistanceFromHome** <br>\n",
    "because it handles the skewness by divide the histogram and make each of divided histogram to Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3014823-e60a-4181-90a2-507d15e3bb38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## T6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54662aab-0658-457a-99a8-7491532de73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_discretize_hist(df, col, bins, ax):\n",
    "    train_col_no_nan = df[~df[col].isna()][col]\n",
    "    hist, bin_edge = np.histogram(train_col_no_nan, bins=bins)\n",
    "    \n",
    "    ax.set_title(f'{col}, bins={bins}')\n",
    "    ax.fill_between(bin_edge.repeat(2)[1:-1], hist.repeat(2), facecolor='steelblue')\n",
    "    \n",
    "    discretized_col = np.digitize(train_col_no_nan, bin_edge)\n",
    "    return discretized_col\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e3f69-a396-45eb-bb0e-03c4dd802941",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bins = [10, 40, 100]\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(25, 20))\n",
    "for i, col in enumerate(cols):\n",
    "    for j, bins in enumerate(all_bins):\n",
    "        plot_discretize_hist(df_train, col, bins, ax[i, j])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7d7b9-40a1-4a7a-9c67-6212c62acd1a",
   "metadata": {},
   "source": [
    "Considering the bins size of each feature by the sparseness of histogram\n",
    "\n",
    "**Age** - bins = 40\n",
    "\n",
    "**MonthlyIncome**  - bins = 40 ( The bins=100 look good but has a little sparse)\n",
    "\n",
    "**DistanceFromHome** - bins = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2aaee-3605-45d8-8de3-5eff0b9094ce",
   "metadata": {},
   "source": [
    "## T7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74c409-a56b-46b0-9d34-f7303546428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = np.setdiff1d(df_train.columns, np.array(cat_cols))\n",
    "for col in num_cols:\n",
    "    train_col_no_nan = df_train[~df_train[col].isna()][col]\n",
    "    hist, bin_edge = np.histogram(train_col_no_nan, bins=10)\n",
    "    if (zero_cnt := (hist == 0).sum()) == 0:\n",
    "        print(f'{col} column has {zero_cnt} bins zero counts ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431604b9-e837-48c0-90bc-07117308b0e9",
   "metadata": {},
   "source": [
    "### Feature should Discretize\n",
    "Age, DailyRate, DistanceFromHome, HourlyRate, JobRole, MonthlyIncome, MonthlyRate, NumCompaniesWorked, PercentSalaryHike, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager\n",
    "\n",
    "because these features have no 0 bins counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da80935-52c7-43f5-8477-959c17223ecc",
   "metadata": {},
   "source": [
    "## T8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020655b-69dd-47d0-b0fd-f30be77bf7bb",
   "metadata": {},
   "source": [
    "Distribution: **Multinomial Distribution**\n",
    "$$\n",
    "X \\sim multinomial(\\textbf{p}, n)\n",
    "$$\n",
    "\n",
    "\n",
    "MLE of Multinomial Distribution:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{\\text{size of bin}_i}{\\#\\text{samples data}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b2cfa-59ea-4cba-98d0-d1a3348baba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_likelihood(df, col, cl, ax):\n",
    "    attrition_match = df[df['Attrition'] == cl]\n",
    "    ax.hist(attrition_match.loc[~attrition_match[col].isna(), col], bins=10)\n",
    "    ax.set_title(f\"{col}, { 'Stay' if cl==0 else 'Leave' }\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d456922-e7a7-4d57-a1c6-d832aac94a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['MonthlyIncome', 'JobRole', 'HourlyRate', 'MaritalStatus']\n",
    "fig, ax = plt.subplots(4, 2, figsize=(15, 20))\n",
    "\n",
    "for i,col in enumerate(cols):\n",
    "    for cl in range(2):\n",
    "        plot_likelihood(df_train, col, cl, ax[i, cl])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02fa6ba-7038-43c4-aad0-634285b5349b",
   "metadata": {},
   "source": [
    "## T9\n",
    "**Binomial Distribution** because Attrition has only two classes (Stay, Leave) same as flipping coin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2a8f2-1464-4602-ac8b-23c2a8a35110",
   "metadata": {},
   "source": [
    "## T10\n",
    "1. Flooring: use small value instead (1e-10, 1e-8)\n",
    "2. Smoothing: smooth the values using counts from other observations (mean of adjacent bin, etc.)\n",
    "3. Use priors (MAP adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e676a4b-c148-4c27-9cce-6a64ec7860f2",
   "metadata": {},
   "source": [
    "## T11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d8f93-b1b2-42d3-8553-2896d1e62220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Binary_BayesClassifier_Hist:\n",
    "    def __init__(self, bins=10, threshold=0):\n",
    "        self.bins = bins\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    \n",
    "    def _discretize_test(self, dat, feature_index):\n",
    "        return np.digitize(dat, self.bin_edges[feature_index])\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        # Calculate prior\n",
    "        self.priors = [np.sum(y == w) / num_samples for w in range(2)]\n",
    "        \n",
    "        bin_edges = []\n",
    "        features_prob = [[], []]\n",
    "        \n",
    "        for feature_index in range(num_features):\n",
    "            non_nan_mask = ~np.isnan(X[:, feature_index])\n",
    "            \n",
    "            cur_feature_no_nan = X[non_nan_mask, feature_index]\n",
    "            cur_class_no_nan = y[non_nan_mask]\n",
    "            \n",
    "            _, bin_edge = np.histogram( cur_feature_no_nan, bins = self.bins )\n",
    "            bin_edge[0], bin_edge[-1] = -np.inf, np.inf # Expand edge\n",
    "            \n",
    "            for w in range(2):\n",
    "                current_feature_class = cur_feature_no_nan[cur_class_no_nan == w]\n",
    "                hist, _ = np.histogram( current_feature_class , bins = bin_edge)\n",
    "                bins_prob = hist / len(current_feature_class)\n",
    "                bins_prob[bins_prob == 0] = 1e-6 # Flooring\n",
    "                features_prob[w].append(bins_prob.tolist())\n",
    "                \n",
    "            bin_edges.append(bin_edge)\n",
    "        \n",
    "        self.bin_edges = np.array(bin_edges)\n",
    "        self.features_prob = np.array(features_prob)\n",
    "        return self\n",
    "            \n",
    "    \n",
    "    def predict(self, _X, threshold = None, get_prob = False):\n",
    "        if threshold == None:\n",
    "            threshold = self.threshold\n",
    "            \n",
    "        X = _X.copy()\n",
    "        # Discretize X\n",
    "        _, num_features = X.shape\n",
    "        for feature_index in range(num_features):\n",
    "            non_nan_mask = ~np.isnan(X[:, feature_index])\n",
    "            X[non_nan_mask, feature_index] = np.digitize(X[non_nan_mask, feature_index], self.bin_edges[feature_index]) # Binning\n",
    "            \n",
    "        prediction = []\n",
    "        for data in X:\n",
    "            lH = np.log(self.priors[1]) - np.log(self.priors[0])\n",
    "            lH += sum([ np.log(self.features_prob[ 1, feature_index, int(data[feature_index]) - 1 ])\n",
    "                       -np.log(self.features_prob[ 0, feature_index, int(data[feature_index]) - 1 ]) \n",
    "                        if not np.isnan(data[feature_index]) else 0\n",
    "                        for feature_index in range(num_features)\n",
    "                      ])\n",
    "            if get_prob:\n",
    "                prediction.append(np.exp(lH))\n",
    "            else:\n",
    "                prediction.append(1 if lH > threshold else 0)\n",
    "\n",
    "        return np.array(prediction)\n",
    "    \n",
    "    def predict_proba(self, _X):\n",
    "        return self.predict(_X, None, get_prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6a901-c54c-4540-819a-8bc0c1ce6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='Attrition').to_numpy()\n",
    "y_train = df_train['Attrition'].to_numpy()\n",
    "X_test = df_test.drop(columns='Attrition').to_numpy()\n",
    "y_test = df_test['Attrition'].to_numpy()\n",
    "\n",
    "classifier_hist = Simple_Binary_BayesClassifier_Hist().fit(X_train, y_train)\n",
    "y_pred = classifier_hist.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2715ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_test, y_pred):\n",
    "    if np.sum(y_pred == 1) == 0:\n",
    "        return 0\n",
    "    return np.sum((y_test == 1) & (y_pred == 1)) / np.sum(y_pred == 1)\n",
    "\n",
    "def recall_score(y_test, y_pred):\n",
    "    if np.sum(y_test == 1) == 0:\n",
    "        return 0\n",
    "    return np.sum((y_test == 1) & (y_pred == 1)) / np.sum(y_test == 1)\n",
    "\n",
    "def f1_score(y_test, y_pred):\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    if(prec + recall == 0): \n",
    "        return 0\n",
    "    return 2 * prec * recall / (prec + recall)\n",
    "\n",
    "def accuracy_score(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "def fpr_rate(y_test, y_pred):\n",
    "    return np.sum((y_test == 0) & (y_pred == 1)) / np.sum(y_test == 0)\n",
    "\n",
    "\n",
    "def roc_curve(y, prob):\n",
    "    thresholds = np.sort( np.block([0, np.unique(prob), 1]) )\n",
    "    fpr, tpr = [], []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = prob >= threshold\n",
    "        fpr.append( fpr_rate(y, y_pred) )\n",
    "        tpr.append( recall_score(y, y_pred) )\n",
    "    return fpr, tpr\n",
    "\n",
    "def confusion_matrix(y_test, y_pred):\n",
    "    return np.array([ \n",
    "            [np.sum((y_test == 0) & (y_pred == 0)), np.sum((y_test == 0) & (y_pred == 1))] , \n",
    "            [np.sum((y_test == 1) & (y_pred == 0)), np.sum((y_test == 1) & (y_pred == 1))]\n",
    "        ])\n",
    "\n",
    "def classification_report(y_test, y_pred):\n",
    "    s = '\\tprecision\\trecall\\t\\tf1-score\\n'\n",
    "    s += f'\\t{precision_score(y_test, y_pred):.2f}\\t\\t{recall_score(y_test, y_pred):.2f}\\t\\t{f1_score(y_test, y_pred):.2f}\\n'\n",
    "    s += f'accuracy: {accuracy_score(y_test, y_pred):.2f}'\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe291ad-fe9c-424d-91d4-4f619441aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True ,fmt='g', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3cb87-7c13-451d-9479-d049de6584e1",
   "metadata": {},
   "source": [
    "## T12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c4973-9280-45c8-81a9-8d763b8f30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Binary_BayesClassifier_Gaussian:\n",
    "    def __init__(self, threshold=0):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        # Calculate prior\n",
    "        self.priors = [np.sum(y == w) / num_samples for w in range(2)]\n",
    "        dists = [[], []]\n",
    "        for feature_index in range(num_features):\n",
    "            cur_data = X[:, feature_index]\n",
    "            for w in range(2):\n",
    "                cur_data_class = cur_data[y == w]\n",
    "                dists[w].append( norm(np.nanmean(cur_data_class), np.nanstd(cur_data_class)) )\n",
    "        self.dists = np.array(dists)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, threshold=None, get_prob=False):\n",
    "        if threshold == None:\n",
    "            threshold = self.threshold\n",
    "            \n",
    "        _, num_features = X.shape\n",
    "        prediction = []\n",
    "        for data in X:\n",
    "            lH = np.log(self.priors[1]) - np.log(self.priors[0])\n",
    "            lH += sum([ np.log( self.dists[1, feature_index].pdf(data[feature_index]) )\n",
    "                       -np.log( self.dists[0, feature_index].pdf(data[feature_index]) )\n",
    "                       if not np.isnan(data[feature_index]) else 0\n",
    "                       for feature_index in range(num_features)\n",
    "                      ])\n",
    "            \n",
    "            if get_prob:\n",
    "                prediction.append(np.exp(lH))\n",
    "            else:\n",
    "                prediction.append(1 if lH > threshold else 0)\n",
    "\n",
    "        return np.array(prediction)\n",
    "    \n",
    "    def predict_proba(self, _X):\n",
    "        return self.predict(_X, None, get_prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469e793-7138-4948-808a-257c083b0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gaussian = Simple_Binary_BayesClassifier_Gaussian().fit(X_train, y_train)\n",
    "y_pred = classifier_gaussian.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ac0cd-6921-4a52-9f4d-2440956c31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True ,fmt='g', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a4775f-02aa-4ab9-b0da-fa200f0cf6c6",
   "metadata": {},
   "source": [
    "## T13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a27ea-cd9a-4eae-8141-0d9c53579f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6d367-231f-4e47-9a16-ed1ad2bde3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_baseline_1 = (np.random.rand(len(y_test)) >= 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred_baseline_1))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_baseline_1), annot=True ,fmt='g', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3fbdd4-55e4-45ac-9db9-4773ec02812d",
   "metadata": {},
   "source": [
    "## T14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65c06a-edc2-4246-a455-f965abd17eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, cnt = np.unique(y_train, return_counts = True)\n",
    "print(unique, cnt, sep='\\n')\n",
    "frequent_class = unique[cnt.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b7e3a-dad6-42e6-a9e1-e342e3ec957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "y_pred_majority_rule = np.full(len(y_test) ,frequent_class)\n",
    "print(classification_report(y_test, y_pred_majority_rule))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_majority_rule), annot=True ,fmt='g', cmap='Blues')\n",
    "\n",
    "warnings.filterwarnings(action='default')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9003fc-d8ab-489f-8cee-93a0b3c05c0a",
   "metadata": {},
   "source": [
    "## T15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4de137-b175-426e-9791-deb3f0b68c51",
   "metadata": {},
   "source": [
    "| Model | Recall | Precision | F1 | Accuracy |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Naïve Bayes Histogram  | .38 | .38 | .38 | .80 |\n",
    "| Naïve Bayes Gaussian   | .27 | .42 | .32 | .72 |\n",
    "| Baseline random        | .13 | .38 | .19 | .49 |\n",
    "| Baseline majority rule | 0   | 0   | 0   | .84 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9de0b-cfc3-4b7d-a784-d5e1eafea5c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## T16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(-5, 5, 0.05)\n",
    "\n",
    "f1_max_hist = (-1, -1)\n",
    "f1_max_gau = (-1, -1)\n",
    "acc_max_hist = (-1, -1)\n",
    "acc_max_gau = (-1, -1)\n",
    "\n",
    "hist_classifier = Simple_Binary_BayesClassifier_Hist().fit(X_train, y_train)    \n",
    "gaussian_classifier = Simple_Binary_BayesClassifier_Gaussian().fit(X_train, y_train)\n",
    "\n",
    "for threshold in t:\n",
    "    \n",
    "    y_pred_hist = hist_classifier.predict(X_test, threshold)\n",
    "    y_pred_gau = gaussian_classifier.predict(X_test, threshold)\n",
    "    \n",
    "    f1_max_hist = max( f1_max_hist, (f1_score(y_test, y_pred_hist), threshold) )\n",
    "    f1_max_gau = max( f1_max_gau, (f1_score(y_test, y_pred_gau), threshold) )\n",
    "    \n",
    "    acc_max_hist = max( acc_max_hist, (accuracy_score(y_test, y_pred_hist), threshold) )\n",
    "    acc_max_gau = max( acc_max_gau, (accuracy_score(y_test, y_pred_gau), threshold) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy max Histogram {acc_max_hist[0]:.2f} at threshold {acc_max_hist[1]:.2f}\")\n",
    "print(f\"Accuracy max Gaussian {acc_max_gau[0]:.2f} at threshold {acc_max_gau[1]:.2f}\")\n",
    "\n",
    "\n",
    "print(f\"F1 max Histogram {f1_max_hist[0]:.2f} at threshold {f1_max_hist[1]:.2f}\")\n",
    "print(f\"F1 max Gaussian {f1_max_gau[0]:.2f} at threshold {f1_max_gau[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37abd3b8-3b58-4723-84c6-d6a8cb87ab84",
   "metadata": {},
   "source": [
    "## T17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44655a-46df-461b-892e-8e255ef3cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(model, X, y, label):\n",
    "    y_pred_prob = model.predict_proba(X)\n",
    "    fpr, tpr = roc_curve(y, y_pred_prob)\n",
    "    plt.plot(fpr, tpr, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b8fc5-79ab-4901-991c-3d30472ea766",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_bins10_classifier = Simple_Binary_BayesClassifier_Hist(bins=10).fit(X_train, y_train)\n",
    "gaussian_classifier = Simple_Binary_BayesClassifier_Gaussian(threshold = threshold).fit(X_train, y_train)\n",
    "\n",
    "plot_roc(hist_bins10_classifier, X_test, y_test, 'Histogram')\n",
    "plot_roc(gaussian_classifier, X_test, y_test, 'Gaussian')\n",
    "\n",
    "plt.axline((0, 0), slope=1, c='k', linestyle = '--', alpha=0.5)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9a54e-72dc-4e30-b026-ebb9effed83a",
   "metadata": {},
   "source": [
    "## T18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070118ae-98a3-4b77-9c3b-6aaad9740d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_bins5_classifier = Simple_Binary_BayesClassifier_Hist(bins=5).fit(X_train, y_train)\n",
    "\n",
    "plot_roc(hist_bins10_classifier, X_test, y_test, 'Histogram bins=10')\n",
    "plot_roc(hist_bins5_classifier, X_test, y_test, 'Histogram bins=5')\n",
    "\n",
    "plt.axline((0, 0), slope=1, c='k', linestyle = '--', alpha=0.5)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate (Recall)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc783ec2-0e6a-490f-ab1f-44833e4d096d",
   "metadata": {},
   "source": [
    "The **Employee Attrition** has consider **recall** over precision because the employee that leave has high affect to the company.\n",
    "\n",
    "Considering to choose bins=10 over bins = 5, at FPR $\\approx$ 0.3 is acceptable and TPR of bins=10 is higher than TPR of bins=5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76244e17-d1b0-4a78-9bd4-e9093f416c37",
   "metadata": {},
   "source": [
    "## T19\n",
    "Submit your code (.py or .ipynb) on mycourseville."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb4c0f-a5a8-4b3e-8f3e-4bec4545908e",
   "metadata": {},
   "source": [
    "## OT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3c6fe-f94c-4041-9127-29fa6702fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_states = list(range(10))\n",
    "\n",
    "acc_hist = []\n",
    "acc_gau = []\n",
    "\n",
    "for random_state in random_states:\n",
    "    df_train, df_test = train_test_split(df, test_size = 0.1, random_state = random_state, stratify = df['Attrition'], shuffle=True)\n",
    "    \n",
    "    X_train = df_train.drop(columns='Attrition').to_numpy()\n",
    "    y_train = df_train['Attrition'].to_numpy()\n",
    "    X_test = df_test.drop(columns='Attrition').to_numpy()\n",
    "    y_test = df_test['Attrition'].to_numpy()\n",
    "    \n",
    "    hist_classifier = Simple_Binary_BayesClassifier_Hist().fit(X_train, y_train)\n",
    "    gaussian_classifier = Simple_Binary_BayesClassifier_Gaussian().fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_hist = hist_classifier.predict(X_test)\n",
    "    y_pred_gau = gaussian_classifier.predict(X_test)\n",
    "    \n",
    "    acc_hist.append(accuracy_score(y_test, y_pred_hist))\n",
    "    acc_gau.append(accuracy_score(y_test, y_pred_gau))\n",
    "\n",
    "acc_hist = np.array(acc_hist)\n",
    "acc_gau = np.array(acc_gau)\n",
    "\n",
    "print(f\"Histogram discretize Naïve Bayes mean = {acc_hist.mean()}, var = {acc_hist.var()}\")\n",
    "print(f\"Gaussian discretize Naïve Bayes mean = {acc_gau.mean()}, var = {acc_gau.var()}\")\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Pana Wanitchollakit 6532136721"
   }
  ],
  "date": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "title": "Attrition Prediction"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
