{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e12e8-4011-484d-97d1-76d8078d358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import scipy\n",
    "import scipy.io\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f58b7e-233a-4be2-84c6-e227a894f99b",
   "metadata": {},
   "source": [
    "# Hello Soft Clustering (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad87f2-9315-4bf6-9acc-d7f65c0509e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel:\n",
    "    def __init__(self, n_mixtures = 3, mode=None):\n",
    "        if mode == \"T1\":\n",
    "            n_mixtures = 3\n",
    "            self.means = np.array([[3, 3], [2, 2], [-3, -3]], dtype=np.float64)\n",
    "            self.verbose = True\n",
    "            \n",
    "        elif mode == \"T3\":\n",
    "            n_mixtures = 2\n",
    "            self.means = np.array([[3, 3],[-3, -3]], dtype=np.float64)\n",
    "            self.verbose = True\n",
    "\n",
    "        elif mode == \"OT1\":\n",
    "            n_mixtures = 2\n",
    "            self.means = np.array([[0,0],[10_000, 10_000]], dtype=np.float64)\n",
    "            self.verbose = True\n",
    "\n",
    "        self.n_mixtures = n_mixtures\n",
    "        self.weights = np.full(n_mixtures, 1 / n_mixtures, dtype=np.float64)\n",
    "\n",
    "    def _expectation_step(self, X):\n",
    "        W = np.vstack([ stats.multivariate_normal(self.means[j], self.covs[j]).pdf(X) * self.weights[j]\n",
    "                        for j in range(self.n_mixtures) ]).T\n",
    "        return W / W.sum(axis=1, keepdims=True) #normalize\n",
    "\n",
    "    def _maximization_step(self, X, W):\n",
    "        self.weights = W.sum(axis=0) / W.shape[0]\n",
    "        self.means = W.T @ X / W.sum(axis=0).reshape(-1, 1)\n",
    "\n",
    "        self.covs = np.stack([\n",
    "            ( (X - self.means[j]).T * W[:, j] ) @ (X - self.means[j]) / W[:, j].sum()\n",
    "            for j in range(self.n_mixtures)\n",
    "        ])\n",
    "        self.covs[:, np.eye(X.shape[1])==0] = 0 # Σ_(i,j) = 0, for i != j.\n",
    "        \n",
    "    def _calculate_likelihood(self, X):\n",
    "        W = np.vstack([ stats.multivariate_normal(self.means[j], self.covs[j]).pdf(X) * self.weights[j]\n",
    "                        for j in range(self.n_mixtures) ]).T\n",
    "            \n",
    "        self.log_likelihoods.append( np.log( W.sum(axis=1) ).sum() )\n",
    "        \n",
    "     \n",
    "    def fit(self, X, epochs=3):\n",
    "        \n",
    "        num_samples, num_features = X.shape\n",
    "        self.covs = np.stack( [ np.eye(num_features, dtype=np.float64) for _ in range(self.n_mixtures) ] )\n",
    "        self.log_likelihoods = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if self.verbose:\n",
    "                print(f\"epoch {epoch+1}:\")\n",
    "                print(127 * '*')\n",
    "                \n",
    "            W = self._expectation_step(X)\n",
    "            self._maximization_step(X, W)\n",
    "\n",
    "            if self.verbose:\n",
    "                self.describe(X, W, num_features)\n",
    "            \n",
    "            self._calculate_likelihood(X)\n",
    "            \n",
    "           \n",
    "        return self\n",
    "\n",
    "    def plot_likelihood(self):\n",
    "        plt.title('log likelihood')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.xticks([1,2,3])\n",
    "        plt.plot(range(1, len(self.log_likelihoods)+1), self.log_likelihoods)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_gauss_iteration(self):\n",
    "        pass\n",
    "        \n",
    "    def describe(self, X, W, num_features):\n",
    "        print(60*'-', 'W_n,j', 60*'-')\n",
    "        for i, w in enumerate(W):\n",
    "            print(f\"({X[i, 0]}, {X[i, 1]}): \", *w, sep='\\t\\t\\t')\n",
    "        print(127 * '-')\n",
    "        \n",
    "        print(61*'-', 'm_j', 61*'-')\n",
    "        print('', *self.weights, sep='\\t\\t\\t')\n",
    "        print(127 * '-')\n",
    "        \n",
    "        print(61*'-', 'μ_j', 61*'-')\n",
    "        print('', *[f'({x:.7f}, {y:.7f})' for x,y in self.means], sep='\\t\\t\\t')\n",
    "        print(127 * '-')\n",
    "        \n",
    "        print(61*'-', 'Σ_j', 61*'-')\n",
    "        for row in range(num_features):\n",
    "            print('', *self.covs[:, row, :] , sep='\\t\\t\\t')\n",
    "        print(127 * '*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc13250-6c08-4143-b245-3ecb4872244f",
   "metadata": {},
   "source": [
    "## T1, T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31574d91-8d7f-4f51-bd0b-8e0261026ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 2], [3, 3], [2, 2], [8, 8], [6, 6], [7, 7], [-3, -3], [-2, -4], [-7, -7]], dtype=np.float64)\n",
    "gmm_t1 = GaussianMixtureModel(mode=\"T1\").fit(data)\n",
    "gmm_t1.plot_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a6bf5-ab1d-4fae-b77d-856333399774",
   "metadata": {},
   "source": [
    "## T3, T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec9b2b-fa6b-403f-8db2-9f22bdf2ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_t3 = GaussianMixtureModel(mode=\"T3\").fit(data)\n",
    "gmm_t3.plot_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42721d07-b75e-400a-9e19-6f9e2b050c20",
   "metadata": {},
   "source": [
    "### T4 ans\n",
    "The three mixture model has higher log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89bf87c-8ca4-43b8-98d5-7b41aec04e6b",
   "metadata": {},
   "source": [
    "## OT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86fcfc1-918a-4259-aa89-1c08f65df4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gmm_ot1 = GaussianMixtureModel(mode=\"OT1\").fit(data)\n",
    "except:\n",
    "    print('Got Nan. Very Sad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a68382-09c8-4ec9-a400-162a031f16ad",
   "metadata": {},
   "source": [
    "### OT1 ans\n",
    "The $m_2$ is equal to $0$ which implies that the model intepret that data are distributed only from 1 mixture. The reason is that initial point is very far from the data points. To prevent this situation, set the initial point on the sample data like the K-mean does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcbdd17-92ca-4363-8a7c-50a6a732e46b",
   "metadata": {},
   "source": [
    "# The face database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04494e-16b7-4f71-95df-aa9d48cb789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('facedata.mat')\n",
    "num_persons, num_images = data['facedata'].shape\n",
    "img_h, img_w = data['facedata'][0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e47238-b50d-466f-bda8-0e20e5775ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import img_as_float\n",
    "\n",
    "xf = { (i, j) : img_as_float(data['facedata'][i, j]) \n",
    "            for i in range(num_persons) \n",
    "            for j in range(num_images) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a4fca-664e-46d9-a0a0-c29252685ae4",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd6830-81a1-4002-9c4e-9ff6e3642b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Euclidean distance between xf[0, 0] and xf[0, 1]', np.sqrt( np.power(xf[0, 0] - xf[0, 1], 2).sum() ) )\n",
    "print('Euclidean distance between xf[0, 0] and xf[1, 0]', np.sqrt( np.power(xf[0, 0] - xf[1, 0], 2).sum() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168850ac-8131-4a00-9cda-ae425e18cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 10))\n",
    "ax[0].imshow(xf[0, 1], cmap='gray')\n",
    "ax[0].set_title('xf[0,1]')\n",
    "\n",
    "ax[1].imshow(xf[0, 0], cmap='gray')\n",
    "ax[1].set_title('xf[0,0]')\n",
    "\n",
    "ax[2].imshow(xf[1, 0], cmap='gray')\n",
    "ax[2].set_title('xf[1, 0]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63aa63-899e-46a7-9bb2-d05fd5640a5c",
   "metadata": {},
   "source": [
    "### T5 ans:\n",
    "The euclidean distance seem doesn't make sense because the same person must has higher similarity than other person. The main reason is $xf[0, 0]$ and $xf[1, 0]$ has share many black spots which yield small distance values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8880e4-e9dc-46be-add2-bf6db2408505",
   "metadata": {},
   "source": [
    "## T6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea9f69-933c-4dc8-9e2f-e2f6905c20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(T, D):\n",
    "    return np.sqrt( np.power(T[:, np.newaxis, :] - D, 2).sum(axis=2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867718d-2c37-4cf7-b984-d4dd801ce61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.array([xf[i, j].flatten() for i in range(num_persons) for j in range(3)])\n",
    "D = np.array([xf[i, j].flatten() for i in range(num_persons) for j in range(3, num_images)])\n",
    "\n",
    "sim_mat = compute_similarity_matrix(T, D)\n",
    "plt.figure(figsize=(28, 12))\n",
    "\n",
    "plt.xlabel('test')\n",
    "plt.ylabel('train')\n",
    "plt.xticks(np.arange(0, 280, 7))\n",
    "plt.yticks(np.arange(0, 120, 3))\n",
    "\n",
    "plt.imshow(sim_mat, cmap='gray')\n",
    "\n",
    "plt.show()\n",
    "print('person 1')\n",
    "pd.DataFrame(sim_mat).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be813495-3eae-4f6d-99d4-3736f1bf9be2",
   "metadata": {},
   "source": [
    "## T7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81546c76-d4c1-49f8-b65d-38c5a45bab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T7 = np.array([xf[i, j].flatten() for i in range(5) for j in range(5)])\n",
    "\n",
    "A = compute_similarity_matrix(T7, T7)\n",
    "plt.imshow(A, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8edb83-2bbf-4ed1-8dba-a4e70e96bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(A[5:10, 5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ceeb4f-57f4-4ec5-9556-db6b7ecaabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( A[:5, :5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5689a-2206-45c4-b20f-936dd2648096",
   "metadata": {},
   "source": [
    "### T7 ans\n",
    "- The black square show that images of person number 2 have high similarity to each other images of himself.\n",
    "- The images of person number 1 has low to medium similarity to himself because the pattern color is in light gray range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c469c-8023-4644-8427-da4ceee62445",
   "metadata": {},
   "source": [
    "# A simple face verification system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8591ee-e52a-488b-878c-707726ddb3e7",
   "metadata": {},
   "source": [
    "## T8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba2c58-6e8f-4041-a50a-e29696acbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_score(y_test, y_pred):\n",
    "    if y_test.sum():\n",
    "        return np.sum((y_test == 1) & (y_pred == 1)) / y_test.sum()\n",
    "    return 0\n",
    "\n",
    "def fpr_rate(y_test, y_pred):\n",
    "    if (1 - y_test).sum():\n",
    "        return np.sum((y_test == 0) & (y_pred == 1)) / (1 - y_test).sum()\n",
    "    return 0\n",
    "    \n",
    "def get_eval(train, test, train_sz, test_sz, n_persons, threshold):\n",
    "    y_pred = _get_prob(train, test, train_sz, test_sz, n_persons) < threshold\n",
    "    y_test = np.repeat(np.eye(n_persons), test_sz, axis=1)\n",
    "    return recall_score(y_test, y_pred), fpr_rate(y_test, y_pred)\n",
    "\n",
    "def _get_prob(train, test, train_sz, test_sz, n_persons):\n",
    "    sim_mat = compute_similarity_matrix(train, test)\n",
    "    prob = sim_mat.reshape(n_persons, train_sz, n_persons, test_sz).min(axis=1).reshape(n_persons, -1)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a089c-f239-468e-a61e-32c762a5de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr = get_eval(T, D, 3, 7, num_persons, 10)\n",
    "print(f\"tpr: {tpr}\", f\"fpr: {fpr}\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead50a8-77ee-43ca-8243-7aba1842e475",
   "metadata": {},
   "source": [
    "## T9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e810b1-1252-4170-aa34-d1e7049e5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(train, test, train_sz, test_sz, n_persons, print_threshold=False):\n",
    "    prob = _get_prob(train, test, train_sz, test_sz, n_persons)\n",
    "    mn, mx = prob.min(), prob.max()\n",
    "    if print_threshold:\n",
    "        print('(min, max) threshold ->', f\"({mn} , {mx})\" )\n",
    "    thresholds = np.linspace(mn, mx, 1000)\n",
    "    y = np.repeat(np.eye(n_persons), test_sz, axis=1)\n",
    "    fpr_s, tpr_s = zip(*[ (fpr_rate(y, y_pred:=prob<threshold), recall_score(y, y_pred)) \n",
    "                        for threshold in thresholds ])\n",
    "    return np.array(fpr_s), np.array(tpr_s), thresholds\n",
    "\n",
    "def plot_roc(train, test, train_sz, test_sz, n_persons, print_threshold=False):\n",
    "    fpr, tpr, thresholds = roc_curve(train, test, train_sz, test_sz, n_persons, print_threshold=print_threshold)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.axline((0, 0), slope=1, c='k', linestyle = '--', alpha=0.5, label='baseline')\n",
    "    plt.axline((1, 0), slope=-1, c='r', label='EER')\n",
    "    \n",
    "    plt.xlabel('false positive(alarm) rate')\n",
    "    plt.ylabel('true positive rate (recall)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63c2ad-f21b-47d7-b423-de75bdf39f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = plot_roc(T, D, 3, 7, num_persons, print_threshold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f247f80-9b24-4b15-9545-13988284720f",
   "metadata": {},
   "source": [
    "## T10\n",
    "Equal Error rate($EER$) $\\Rightarrow$ False Alarm Rate $=$ False Negative Rate\n",
    "\\begin{align*}\n",
    "FAR &= FRR \\\\\n",
    "FAR &= 1 - TPR \\\\\n",
    "FAR + TPR - 1 &= 0\n",
    "\\end{align*}\n",
    "use this to find FAR and TPR, and \n",
    "\n",
    "$$EER = \\frac{FAR + FRR}{2} = \\frac{FAR + (1 - TPR)}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde800f-0cc7-4f30-b428-fdaea4d4708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eer(tpr, fpr, thresholds=None, return_eer=False):\n",
    "    mn_idx = np.argmin(np.abs(tpr + fpr - 1))\n",
    "    far=fpr[mn_idx]\n",
    "    frr=1-tpr[mn_idx]\n",
    "    eer=np.mean((far, frr))\n",
    "    \n",
    "    if return_eer:\n",
    "        return eer\n",
    "        \n",
    "    print('FAR:', far)\n",
    "    print('FRR:', frr)\n",
    "    print(30*'-')\n",
    "    print('EER:', eer)\n",
    "    print('EER Threshold', thresholds[mn_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2cde8d-3548-4a91-bd3c-bfa302f56f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_eer(tpr, fpr, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb316af-2c10-469a-ad2c-985ca018f6b6",
   "metadata": {},
   "source": [
    "### False Alarm Rate at reall 0.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d9198-a6e2-4dc0-a602-dab479364b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_far_rate(tpr, fpr, rate, thresholds=None, return_recall=False):\n",
    "    fpr_idx = np.argmin(np.abs(fpr - rate))\n",
    "    if return_recall:\n",
    "        return tpr[fpr_idx]\n",
    "    print('recall rate at far 0.1%:', tpr[fpr_idx])\n",
    "    print('At threshold:', thresholds[fpr_idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ef3eb-d691-4d55-bb7c-9d38648d0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_at_far_rate(tpr, fpr, .001, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ee008-103a-448b-9846-33bc89f21300",
   "metadata": {},
   "source": [
    "# Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a72edf3-b348-4f04-83d3-95d8180e6046",
   "metadata": {},
   "source": [
    "## T11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c9ab2-d461-4ec2-be39-9335d604d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_face = T.mean(axis = 0)\n",
    "plt.imshow(mean_face.reshape(img_h, img_w) ,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36691ddd-21ad-4dc5-96e1-4acda98b9453",
   "metadata": {},
   "source": [
    "## T12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c8c2a-04e2-4378-8c9c-260371a51264",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = T.T\n",
    "X_hat = X - mean_face.reshape(-1, 1)\n",
    "cov_mat = np.cov(X)\n",
    "print('Covariance Matrix size =', cov_mat.shape)\n",
    "print('Covariance Matrix rank =', np.linalg.matrix_rank(cov_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399941e3-31a4-465a-84a5-45a929a4fe65",
   "metadata": {},
   "source": [
    "## T13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9788b-4b61-4e4f-9c8e-25edf676a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_mat = X_hat.T @ X_hat\n",
    "print('Gram Matrix size =', gram_mat.shape)\n",
    "print('Gram Matrix rank =', np.linalg.matrix_rank(gram_mat))\n",
    "\n",
    "eigen_vals, eigen_vecs = np.linalg.eigh(gram_mat) \n",
    "\n",
    "# filter out 0 eigen vals\n",
    "eigen_vecs = eigen_vecs[:, eigen_vals > 1e-6]\n",
    "eigen_vals = eigen_vals[eigen_vals > 1e-6]\n",
    "\n",
    "print('Gram Matrix non-zero eigen value =', len(eigen_vals) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a6a97-6215-4b64-acf5-f2856705963b",
   "metadata": {},
   "source": [
    "## T14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299764fe-312f-4881-9c34-f0ba2ef6adf0",
   "metadata": {},
   "source": [
    "Yes, The gram matrix is symmetric.\n",
    "\\begin{align*}\n",
    "\\hat{X}^T\\hat{X} &= \n",
    "\\begin{bmatrix}\n",
    "\\rule[.5ex]{2.5ex}{0.5pt} & x_1 & \\rule[.5ex]{2.5ex}{0.5pt} \\\\\n",
    "\\rule[.5ex]{2.5ex}{0.5pt} & x_2 & \\rule[.5ex]{2.5ex}{0.5pt} \\\\\n",
    "& \\vdots & \\\\\n",
    "\\rule[.5ex]{2.5ex}{0.5pt} & x_N & \\rule[.5ex]{2.5ex}{0.5pt} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mid & \\mid &  & \\mid \\\\\n",
    "x_1 & x_2 & \\ldots & x_N\\\\\n",
    "\\mid & \\mid &  & \\mid\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "x_1^Tx_1 & x_1^Tx_2 &  & x_1^Tx_N \\\\\n",
    "x_2^Tx_1 & x_2^Tx_1 &  & x_2^Tx_N \\\\\n",
    "  & \\vdots &  \\\\\n",
    "x_N^Tx_1 & x_N^Tx_1 &  & x_N^Tx_N \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$ \\text{and we know that } x_i^Tx_j=x_j^Tx_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc018341-92dc-4c50-9516-4c57e30c134e",
   "metadata": {},
   "source": [
    "## T15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea658e6f-8284-45da-89da-c105043595c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = np.argsort(eigen_vals)[::-1]\n",
    "\n",
    "eigen_vals = eigen_vals[sorted_idx]\n",
    "eigen_vecs = eigen_vecs[:, sorted_idx]\n",
    "\n",
    "print('number of non-zero eigen value:', len(eigen_vals) )\n",
    "print('highest eigen val:', eigen_vals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593cdb4d-b66e-4f9b-a09c-c8d99c616aa9",
   "metadata": {},
   "source": [
    "## T16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2892998-1804-44fa-bc9c-a326255fd041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_eigen_amt(eigen_vals, var):\n",
    "    l, r = 0, len(eigen_vals)+1\n",
    "    while l < r:\n",
    "        m = (l+r)>>1\n",
    "        if _compute_variance(eigen_vals, m) >= var:\n",
    "            r = m\n",
    "        else: \n",
    "            l = m+1\n",
    "    return l\n",
    "def _compute_variance(eigen_vals, amt):\n",
    "    return eigen_vals[:amt].sum() / eigen_vals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63180b3b-a920-475e-a542-9b0e85a212a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eigen_vals)\n",
    "plt.ylabel('eigen values')\n",
    "plt.xlabel('eigen values amount')\n",
    "\n",
    "print('Amount of eigen vectors =', amt:=search_eigen_amt(eigen_vals, .95) )\n",
    "print('Var=', _compute_variance(eigen_vals, amt))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa445f-01b1-48a4-87e7-0951b934565f",
   "metadata": {},
   "source": [
    "## T17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca4848-332e-4405-84ad-e5339f20d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = X_hat @ eigen_vecs\n",
    "v = v / np.linalg.norm(v, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        ax[i, j].set_title(f\"eigen face No. {5*i+j+1}\")\n",
    "        ax[i, j].imshow(v[:, 5*i+j].reshape(img_h, img_w), cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0050de-737c-4dec-9a90-6a62734ca665",
   "metadata": {},
   "source": [
    "## T18\n",
    "The darker pixel show that there is high value in eigen face.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64359744-a76e-40cc-8d35-e6015195c88c",
   "metadata": {},
   "source": [
    "- Eigenface No.1<br>\n",
    "    - The dark part is hair<br>\n",
    "- Eigenface No.2<br>\n",
    "    - The dark part is also hair but there also have eye and collar that have dark part<br>\n",
    "\n",
    "let see the sample of people<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe16d5-7ef8-4f12-a651-d6aa6607b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 8, figsize=(15, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(8):\n",
    "        ax[i, j].set_title(f'Person {8*i+j+1}')\n",
    "        ax[i, j].imshow(xf[8*i+j, 0], cmap='gray')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c3bcc-a8a4-42c8-a797-a18d4823f835",
   "metadata": {},
   "source": [
    "The first and second eigen face are capture the biggest variance of the images because many people have different hair color and style and size of eyes. There also images of people have collar in its so the eigen face will capture it too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631629f7-5057-4693-b38c-1f65f9e88806",
   "metadata": {},
   "source": [
    "# PCA subspace and the face verification system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8435e3-c476-46f4-a7c8-e9c026466d87",
   "metadata": {},
   "source": [
    "## T19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da81fb-f7b9-43d0-9584-7dfec5e4a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimension(X, v, mean_face, k):\n",
    "    '''\n",
    "        X is matrix that have vector in column\n",
    "    '''\n",
    "    V = v[:, :k]\n",
    "    projection = V.T @ (X - mean_face.reshape(-1, 1))\n",
    "    return projection\n",
    "\n",
    "def face_verification_pca(train, test, train_sz, test_sz, n_persons, ks, return_roc=False):\n",
    "    mean_face = train.mean(axis = 0)\n",
    "    X_hat = (train - mean_face).T\n",
    "    gram_mat = X_hat.T @ X_hat\n",
    "\n",
    "    # Compute eigen value\n",
    "    eigen_vals, eigen_vecs = np.linalg.eigh(gram_mat) \n",
    "    eigen_vecs = eigen_vecs[:, eigen_vals > 1e-6]\n",
    "    eigen_vals = eigen_vals[eigen_vals > 1e-6]\n",
    "    # Sort by eigen value\n",
    "    sorted_idx = np.argsort(eigen_vals)[::-1]\n",
    "    eigen_vals = eigen_vals[sorted_idx]\n",
    "    eigen_vecs = eigen_vecs[:, sorted_idx]\n",
    "    \n",
    "    # compute and normalize v\n",
    "    v = X_hat @ eigen_vecs\n",
    "    v = v / np.linalg.norm(v, axis=0)\n",
    "\n",
    "    eers = np.empty(len(ks))\n",
    "    recall_at_far = np.empty(len(ks))\n",
    "\n",
    "    for i, k in enumerate(ks):\n",
    "        train_reduced = reduce_dimension(train.T, v, mean_face, k).T\n",
    "        test_reduced = reduce_dimension(test.T, v, mean_face, k).T\n",
    "        fpr, tpr, _ = roc_curve(train_reduced, test_reduced, train_sz, test_sz, n_persons)\n",
    "\n",
    "        eers[i] = compute_eer(tpr, fpr, return_eer=True)\n",
    "        recall_at_far[i] = recall_at_far_rate(tpr, fpr, .001, return_recall = True)\n",
    "    if return_roc:\n",
    "        return fpr, tpr\n",
    "    return eers, recall_at_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98339ab7-a3ca-4309-9ea7-8901f43d338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eers, recall_001 = face_verification_pca(T, D, 3, 7, num_persons, [10] )\n",
    "print(\"EER at k=10:\", *eers)\n",
    "print(\"Recall at FAR 0.1%:\", *recall_001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b62e375-6917-4ec6-87c7-e0604d9f218a",
   "metadata": {},
   "source": [
    "## T20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0e089-076f-4b06-a717-5dc368816dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(5, 15)\n",
    "eers, _ = face_verification_pca(T, D, 3, 7, num_persons, ks)\n",
    "print('Min eer at', eers.min(), 'at k =', ks[eers.argmin()])\n",
    "pd.DataFrame([eers], columns=ks)\n",
    "\n",
    "plt.plot(ks, eers)\n",
    "plt.xticks(ks)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('EER')\n",
    "plt.show()\n",
    "pd.DataFrame([eers], columns=ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1504c72-49e1-49cd-a690-f17bca28396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_pca, tpr_pca = face_verification_pca(T, D, 3, 7, num_persons, [11], return_roc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c69d1c-908c-4d29-8524-f14023e00959",
   "metadata": {},
   "source": [
    "# (Optional) PCA reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52e6de-a218-4a5d-957e-85856bd90170",
   "metadata": {},
   "source": [
    "## OT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536603ba-848d-42dc-8c5c-dd1092ab0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squarred_error(a, b):\n",
    "    return np.mean((a-b)**2)\n",
    "    \n",
    "def reconstruct_image(mean_face, v, k, projected):\n",
    "    return mean_face + v[:, :k] @ projected[:k]\n",
    "\n",
    "k = 10\n",
    "projected_image = v.T @ (T[0] - mean_face)\n",
    "reconstructed = reconstruct_image(mean_face, v, k, projected_image)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow( reconstructed.reshape(img_h, img_w), cmap='gray')\n",
    "ax[0].set_title('Reconstrcuted')\n",
    "ax[1].imshow( T[0].reshape(img_h, img_w), cmap='gray')\n",
    "ax[1].set_title('Original')\n",
    "\n",
    "print('MSE:', mean_squarred_error(reconstructed, T[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfd6e5-adaa-4390-9b69-63f73492e16c",
   "metadata": {},
   "source": [
    "## OT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478ac70-cd41-4f00-bd7b-95b0b0c30901",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.block([np.arange(1, 11), 119])\n",
    "fig, ax = plt.subplots(4, 3, figsize=(6, 8))\n",
    "fig.tight_layout()\n",
    "\n",
    "mse_list = np.empty_like(ks, dtype=np.float64)\n",
    "for i, k in enumerate(ks):\n",
    "    projected_image = v[:, :k].T @ (T[0] - mean_face)\n",
    "    reconstructed_img = reconstruct_image(mean_face, v, k, projected_image)\n",
    "    ax[i//3, i%3].imshow(reconstructed_img.reshape(img_h, img_w), cmap='gray')\n",
    "    ax[i//3, i%3].set_title(f\"k={k}\")\n",
    "    ax[i//3, i%3].set_axis_off()\n",
    "    mse_list[i] = mean_squarred_error(reconstructed_img, T[0])\n",
    "    \n",
    "ax[3, 2].set_title('Original')\n",
    "ax[3, 2].imshow(T[0].reshape(img_h, img_w), cmap='gray')\n",
    "ax[3, 2].set_axis_off()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(ks, mse_list)\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('k')\n",
    "plt.xticks(ks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b126be-1ce4-49da-9a59-0b3626888dd3",
   "metadata": {},
   "source": [
    "## OT4\n",
    "### Original database\n",
    "total image pixel is $2576$ pixels each. $\\\\$\n",
    "$1$ pixel use $32$ bits float datatype. $\\\\$\n",
    "$1$ image use $32$ $\\times$ $2576$ = $82,432$ bits = $10,304$ bytes per image $\\\\$\n",
    "we have total $1,000,000$ images, therefore we use $10,304$ $\\times$ $10^6$ bytes = $9.5963$ Gibibytes ($10.304$ Gigabytes)\n",
    "\n",
    "### PCA database\n",
    "After projected $2576$ features(pixels)/image $\\rightarrow$ $10$ features/image $\\\\$\n",
    "All images data $(10 \\times 10^6) \\times 4$ bytes = 40,000,000 bytes $\\\\$\n",
    "Store total $10$ eigen faces and $1$ mean face = $\\left((10 + 1) \\times 2576\\right) \\times 4$ btyes = 113,344 bytes $\\\\$\n",
    "$\\therefore$ total memory usage = $40,113,344$ bytes = $38.2551$ Mebibytes ($40.1133$ Megabytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffd195-1a85-4c43-8ced-4ae0675c4c6f",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21283eab-465b-4b22-b605-78fad6f7d9d3",
   "metadata": {},
   "source": [
    "## T21\n",
    "$S_W$ has $N-C$ rank due to we compute $C$ mean for each class. For our example $N-C$ is $120 - 40 = 80$ (120 training images with 40 different people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e6800-07cb-400e-8cf0-cde4aade0e87",
   "metadata": {},
   "source": [
    "## T22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8956c5-240a-41b9-b431-247a5b511737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sw(projected_train, num_classes, data_per_class):\n",
    "    projected_by_class = projected_train.T.reshape(num_classes, data_per_class, -1)\n",
    "    mean_face_by_class = projected_by_class.mean(axis=1, keepdims=True)\n",
    "    deviation = (projected_by_class - mean_face_by_class).reshape(num_classes * data_per_class, -1).T # turn back to 80 * 120\n",
    "    return deviation @ deviation.T\n",
    "    \n",
    "def compute_sb(projected_train, num_classes, data_per_class):\n",
    "    global_mean_faces  = projected_train.mean(axis=1)\n",
    "    projected_by_class = projected_train.T.reshape(num_classes, data_per_class, -1)\n",
    "    mean_face_by_class = projected_by_class.mean(axis=1)\n",
    "    mean_deviation = (mean_face_by_class - global_mean_faces).T\n",
    "    return mean_deviation @ mean_deviation.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7514d1c-122a-4ff0-a358-90a1ff37e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_rank = T.shape[0] - num_persons\n",
    "projected_train = v[:, :sw_rank].T @ (T - mean_face).T\n",
    "\n",
    "sw = compute_sw(projected_train, num_persons, 3)\n",
    "sb = compute_sb(projected_train, num_persons, 3)\n",
    "\n",
    "lda_proj = np.linalg.inv(sw) @ sb\n",
    "is_symmetric = (lda_proj-lda_proj.T < 1e-6).all()\n",
    "\n",
    "print(\"lda proj is\", \"symmetric.\" if is_symmetric else \"asymmetric.\", end=' ')\n",
    "print(\"Therefore, we\", \"can\" if is_symmetric else \"can not\", \"use np.linalg.eigh.\")\n",
    "\n",
    "lda_eigen_vals, lda_eigen_vecs = np.linalg.eig(lda_proj)\n",
    "lda_eigen_vecs = lda_eigen_vecs.real # only real part\n",
    "\n",
    "lda_eigen_vecs = lda_eigen_vecs[:, lda_eigen_vals > 1e-6]\n",
    "lda_eigen_vals = lda_eigen_vals[lda_eigen_vals > 1e-6]\n",
    "sorted_idx = np.argsort(lda_eigen_vals)[::-1]\n",
    "\n",
    "lda_eigen_vecs = lda_eigen_vecs[:, sorted_idx]\n",
    "lda_eigen_vals = lda_eigen_vals[sorted_idx]\n",
    "\n",
    "\n",
    "print(\"sw rank\", np.linalg.matrix_rank(sw))\n",
    "print(\"sb rank\", np.linalg.matrix_rank(sb))\n",
    "print(\"lda proj rank\", np.linalg.matrix_rank(lda_proj))\n",
    "\n",
    "print(\"number of non zero eigen values\", len(lda_eigen_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c2dcf-0920-477b-8aaa-33a4c12c726b",
   "metadata": {},
   "source": [
    "## T23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1ef0f-9917-421b-be70-6d85c8f8b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda eigen -> pca -> image\n",
    "lda_eigen_constructed = v[:, :sw_rank] @ lda_eigen_vecs\n",
    "print(lda_eigen_constructed.shape)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('LDA eigenface')\n",
    "\n",
    "for k in range(10):\n",
    "    i, j = k // 5, k % 5\n",
    "    ax[i, j].imshow(lda_eigen_constructed[:, k].reshape(img_h, img_w), cmap='gray_r')\n",
    "    ax[i, j].set_title(f\"k={k+1}\")\n",
    "    ax[i, j].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4718cb3-2f04-4668-9ea3-d285ed2acd7e",
   "metadata": {},
   "source": [
    "LDA eigenface tell what different between each people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09cbf2-2596-4a40-b0a2-954ab3b71295",
   "metadata": {},
   "source": [
    "## T24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e6a0d-1ebb-4823-ae63-7f9af12bba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_train_lda = lda_eigen_constructed.T @ T.T\n",
    "projected_test_lda = lda_eigen_constructed.T @ D.T\n",
    "\n",
    "fpr_lda, tpr_lda, _ = roc_curve(projected_train_lda.T, projected_test_lda.T, 3, 7, num_persons)\n",
    "eer = compute_eer(tpr_lda, fpr_lda, return_eer=True)\n",
    "recall = recall_at_far_rate(tpr_lda, fpr_lda, .001, return_recall = True)\n",
    "\n",
    "print(\"EER\", eer)\n",
    "print(\"Recall at FAR 0.1%:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a6914-cbe0-44c6-8be6-408b35e0a87b",
   "metadata": {},
   "source": [
    "## T25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83885f7b-b2f0-4978-9375-227022c2e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, label='No projection')\n",
    "plt.plot(fpr_pca, tpr_pca, label='PCA')\n",
    "plt.plot(fpr_lda, tpr_lda, label='LDA')\n",
    "\n",
    "plt.axline((0, 0), slope=1, c='k', linestyle = '--', alpha=0.5, label='baseline')\n",
    "plt.axline((1, 0), slope=-1, c='r', label='EER')\n",
    "\n",
    "\n",
    "plt.xlabel('false positive(alarm) rate')\n",
    "plt.ylabel('true positive rate (recall)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465de8c4-cc85-40a1-af49-5af6a1e401e5",
   "metadata": {},
   "source": [
    "LDA has the best performance. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e9cb3-f001-4bbf-bc82-908594affa5b",
   "metadata": {},
   "source": [
    "## OT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8ac21-fcd7-44c1-ac9a-c0673a4b2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster(projected_data, samples_size, img_per_sample):\n",
    "    plt_dim = projected_data[:2, :samples_size * img_per_sample].T\n",
    "    plt_x, plt_y = plt_dim[:, 0], plt_dim[:, 1]\n",
    "    plt_x = plt_x.reshape(samples_size, img_per_sample)\n",
    "    plt_y = plt_y.reshape(samples_size, img_per_sample)\n",
    "    color = ['g', 'r', 'b', 'purple', 'magenta', 'orange', 'cyan']\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(samples_size):\n",
    "        plt.scatter(plt_x[i], plt_y[i], c=color[i], label=str(i+1))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e867a9f9-a348-4cc8-bb1d-dab8a7d2e5f1",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba0b3f-37ab-480b-ad5c-263062f1a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(projected_test_lda, 6, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765def4f-ffc5-49a3-9a39-c6d3be36f830",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f4e10-fe66-4066-88a1-31e27c1564c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_pca = reduce_dimension(D.T, v, mean_face, 2)\n",
    "plot_cluster(projected_pca, 6, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7e7c8-3966-46ef-afbf-2402c640cee3",
   "metadata": {},
   "source": [
    "The result seem that PCA is better cluster than LDA (not as my expected). I think the main reason is our LDA is compressed from the PCA (to make $S_w$ invertible, non-singular)."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Pana Wanitchollakit 6532136721"
   }
  ],
  "date": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "title": "Homework 3 Fisherface"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
